# Big Data
global_hdfs: no

# Directory in the server to store files to transfer/transferred to/from global HDFS
global_hdfs_data_dir: $HOME/ssd/transfer
global_hdfs_disk_name: "ssd_hdfs"

# reserve server node to exclusively deploy master containers for global hdfs and regular hadoop apps.
# only applies if 'server_as_host' parameter is enabled
reserve_server_for_master: yes

local_namenode_port: 55555
namenode_port: 9870

default_hdfs_replication: 3
global_hdfs_replication: "{{ default_hdfs_replication }}"
local_hdfs_replication: 1

# data size threshold when moving data between global and local HDFSs
## if data size is lower than threshold  --> run a simple get/put data transfer
## if data size is higher than threshold --> run distcp
## by default the threshold is set to 2g to leverage the tmpfs of the containers and avoid disk I/O
distcp_threshold: 2g

# mode of assigning the 'blocksperchunk' parameter of distcp, that will modulate the number of mappers created
# 'best-effort': it will try to assign the best distribution
# 'distcp-default': 0 (do not slice files, create 1 mapper per file)
# 'mapred-default': 1 (slice per every block, create 1 mapper per block)
# 'datanode-mapping': try to assign a mapper per datanode;
  ## this is a best-effort estimation, since it depends on the number of blocks of the selected path
  ## an unbalanced block distribution in the files may lower the precision of the resulting distribution
mappers_assignation: 'best-effort'