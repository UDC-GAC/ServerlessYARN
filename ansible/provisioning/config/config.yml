## Virtual mode: "yes" to deploy a vagrant virtual cluster; "no" if you want to use an existing cluster
virtual_mode: yes

## Currently supported container engines: lxc, apptainer
container_engine: apptainer
singularity_command_alias: apptainer # singularity or apptainer (only relevant when using apptainer as container engine)

## Cgroups version: v1 (Lecacy or Hybrid mode) or v2 (Unified mode)
# Apptainer + cgroups V1 requires sudo privileges to some singularity commands at the moment
cgroups_version: v1

## Sampling frequency (in seconds) of all the monitoring tools involved in the platform (BDWatchdog, PowerSender,...)
sampling_frequency: 5

## Server Node
server_ip: 192.168.56.200
cpus_server_node: 2
memory_server_node: 4096
web_interface_port: 9000

## Services Config
orchestrator_url: "{{ server_ip }}"
orchestrator_port: 5000
wattwizard_url: "{{ server_ip }}"
wattwizard_port: 7777
couchdb_url: "{{ server_ip }}"
couchdb_port: 5984
opentsdb_url: "{{ server_ip }}"
opentsdb_port: 4242
mongodb_url: "{{ server_ip }}"
mongodb_port: 27017

## Additional capabilites
# Disk
disk_capabilities: no # "yes" to allow disk capabilities, such as Logical Volumes creation
disk_scaling: no # "yes" to allow disk bandwidth scaling (only relevant when using disk capabilities)
# Power budgeting
power_budgeting: no # "yes" to use power budgets, power will be accounted, restricted and shared among containers
power_modelling: no # "yes" to use power models to get CPU Usage/Power proportionalities instead of using fixed values
online_learning: no # "yes" to automatically retrain power models in real-time using OpenTSDB time series
power_meter: "rapl" # rapl or smartwatts, use the latter to measure power from applications composed of multiple containers
# Big Data
global_hdfs: yes

## WattWizard Config
structures: "host"
model_variables: "user_load,system_load"
prediction_methods: "polyreg,sgdregressor"
modelled_cpu: "intel_xeon_silver_4216"
train_files: "General"
guardian_default_model: "polyreg_General"

## InfluxDB Config
influxdb_host: "montoxo.des.udc.es"
influxdb_bucket: "compute2"
influxdb_token: "MyToken"
influxdb_org: "MyOrg"

## Hosts
number_of_hosts: 2 # 1
cpus_per_host: 4
memory_per_host: 4096
energy_per_host: 200
# Disk config (only relevant if disk_capabilities is set to "yes")
hdd_disks_per_host: 1
hdd_disks_path_list: $HOME/hdd
ssd_disks_per_host: 1
ssd_disks_path_list: $HOME/ssd
create_lvm: no # "yes" to create a LVM using the previous disks. Such disks should be indicated using their /dev path
lvm_path: $HOME/lvm

## Containers initial values
number_of_containers_per_node: 1
# CPU
max_cpu_percentage_per_container: 200
min_cpu_percentage_per_container: 50
cpu_boundary: 10
cpu_boundary_type: "percentage_of_max"
# Memory
max_memory_per_container: 2048
min_memory_per_container: 512
mem_boundary: 15
mem_boundary_type: "percentage_of_max"
# Energy
max_energy_per_container: 40
min_energy_per_container: 10
energy_boundary: 5
energy_boundary_type: "percentage_of_max"
# Disk (only relevant if disk_scaling is set to "yes")
# Disk read
max_disk_read_bw_per_container: 100
min_disk_read_bw_per_container: 50
disk_read_boundary: 10
disk_read_boundary_type: "percentage_of_max"
# Disk write
max_disk_write_bw_per_container: 100
min_disk_write_bw_per_container: 50
disk_write_boundary: 10
disk_write_boundary_type: "percentage_of_max"


## Container network configuration
# host interface through which the containers of different nodes communicate with each other
iface: "{{ 'eth1' if virtual_mode else 'eno1' }}"
# currently supported modes: bridge, ptp, macvlan, ipvlan
mode: ipvlan
subnet: 10.22.0.0/16

## Apps
# List of application configurations to load on startup, the provided name must match the name of an app folder
# in the ansible/provisioning/apps directory. Indicate multiple names in the app_config.yml of the app folder(s)
# specified here to deploy various applications with the same configuration
apps: sample_app
