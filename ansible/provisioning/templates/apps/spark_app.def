Bootstrap: localimage
From: {{ installed_app_directory }}/../{{ app_base_type }}/{{ app_base_image_file }}

%files
    # To avoid repeating spark download while debugging
    #"{{ installed_app_directory }}/spark-{{ spark_version }}-bin-without-hadoop.tgz" /opt/spark-{{ spark_version }}-bin-without-hadoop.tgz

%post
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

    # Setup Spark
    cd /opt
    export SPARK_CONF_DIR=/opt/hadoop_files/spark_conf

    ## Download and uncompress
    spark_download_dir=https://archive.apache.org/dist/spark/spark-{{ spark_version }}
    spark_download_file=spark-{{ spark_version }}-bin-without-hadoop.tgz

    ### Take advantage of the parallel script included in BDWatchdog to speed up the download
    parallel_script_path=/opt/BDWatchdog/deployment/metrics/parallel_curl.sh
    number_of_chunks=$(( $(nproc) * 2 ))
    bash $parallel_script_path $spark_download_dir/$spark_download_file $spark_download_file "${number_of_chunks}"

    mkdir spark && tar xf $spark_download_file -C spark --strip-components 1 && rm $spark_download_file
    chown -R $(whoami):$(whoami) spark

    ## Setup default configuration files
    mkdir -p $SPARK_CONF_DIR && cp -r spark/conf/* $SPARK_CONF_DIR/

%environment
    # Spark
    export SPARK_HOME=/opt/spark
    export SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)
    export SPARK_CONF_DIR="{{ bind_dir_on_container }}/hadoop_files/spark_conf"
