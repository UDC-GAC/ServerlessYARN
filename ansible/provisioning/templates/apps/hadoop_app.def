Bootstrap: localimage
From: {{ installed_app_directory }}/../{{ app_base_type }}/{{ app_base_image_file }}

%files
    {{ installed_app_directory }}/hadoop_files /opt/hadoop_files
    {{ installed_app_directory }}/setup.sh /opt/setup.sh
    {{ installed_app_directory }}/java_snitch.sh /opt/java_snitch.sh

    # To avoid repeating hadoop download while debugging
    #"{{ installed_app_directory }}/hadoop-{{ hadoop_version }}.tar.gz" /opt/hadoop-{{ hadoop_version }}.tar.gz

%post
    ############################ Install basic software ######################################
    export DEBIAN_FRONTEND=noninteractive
    apt-get -y update
    apt-get install -y curl wget gettext-base

    # Trying to setup firewall (iptables prevent hadoop cluster from working)
    #apt-get remove -y iptables
    #apt-get install -y ufw kmod
    #apt-get install -y nftables
    #apt-get install -y --reinstall linux-headers-$(uname -r)
    #apt-get -y update && apt-get -y upgrade
    #modprobe iptable_filter
    #modprobe /lib/modules/$(uname -r)/kernel/net/ipv4/netfilter/iptable_filter.ko
    #ufw --force enable

    ##################################### SSH ###############################################
    apt-get install -y openssh-client openssh-server
    mkdir /run/sshd
    chmod 0711 /run/sshd

    ##################################### Java ##############################################
    apt-get install -y openjdk-8-jdk
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

    ##################################### Hadoop ############################################
    cd /opt
    export HADOOP_CONF_DIR=/opt/hadoop_files/hadoop_conf
    export DATA_DIR="{{ bind_dir_on_container }}/hadoop_files/data_dir"
    mkdir -p hadoop_files/data_dir

    ## Download and uncompress
    hadoop_download_file=hadoop-{{ hadoop_version }}.tar.gz
    {% if local_data_server -%}
    ### Take advantage of the local data server to speed up the download
    wget -O $hadoop_download_file "http://${DATA_SERVER_IP}:${DATA_SERVER_PORT}/$hadoop_download_file"
    {%- else -%}
    ### Take advantage of the parallel script included in BDWatchdog to speed up the download
    parallel_script_path=/opt/BDWatchdog/deployment/metrics/parallel_curl.sh
    number_of_chunks=$(( $(nproc) * 2 ))
    hadoop_download_dir=https://archive.apache.org/dist/hadoop/core/hadoop-{{ hadoop_version }}
    bash $parallel_script_path $hadoop_download_dir/$hadoop_download_file $hadoop_download_file "${number_of_chunks}"
    {%- endif %}

    mkdir hadoop && tar xf $hadoop_download_file -C hadoop --strip-components 1 && rm $hadoop_download_file
    chown -R $(whoami):$(whoami) hadoop

    ## Setup format script and default configuration files
    cp hadoop_files/config/format_filesystem.sh hadoop
    mkdir -p $HADOOP_CONF_DIR && cp -r hadoop/etc/hadoop/* $HADOOP_CONF_DIR/
    envsubst < hadoop_files/config/hdfs-site.xml > $HADOOP_CONF_DIR/hdfs-site.xml
    cp hadoop_files/config/core-site.xml $HADOOP_CONF_DIR/core-site.xml
    cp hadoop_files/config/mapred-site.xml $HADOOP_CONF_DIR/mapred-site.xml
    cp hadoop_files/config/yarn-site.xml $HADOOP_CONF_DIR/yarn-site.xml
    cp hadoop_files/config/workers $HADOOP_CONF_DIR/workers

    ## Set regular user as owner of required files
    chown -R {{ user_info.user_name }} hadoop_files

    ##################################### Zookeper ##########################################
    ## Zookeeper is required for global HDFS rbf mode
    cd /opt
    zookeeper_download_file=apache-zookeeper-{{ zookeeper_version }}-bin.tar.gz
    zookeeper_download_dir=https://dlcdn.apache.org/zookeeper/zookeeper-{{ zookeeper_version }}

    # Use simple wget since the file to download is lightweight
    wget -O $zookeeper_download_file $zookeeper_download_dir/$zookeeper_download_file

    mkdir zookeeper && tar xf $zookeeper_download_file -C zookeeper --strip-components 1 && rm $zookeeper_download_file
    chown -R $(whoami):$(whoami) zookeeper

    ## Setup config
    zoo_conf_file=zookeeper/conf/zoo.cfg
    touch $zoo_conf_file
    for line in \
        "tickTime=2000" \
        "dataDir={{ user_home_on_container }}/zookeeper/data" \
        "clientPort=2181" \
        "initLimit=5" \
        "syncLimit=2"; do
        echo "$line" >> "$zoo_conf_file"
    done

%environment
    # Java
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    export JAVA_MAPPINGS_FOLDER_PATH="{{ bind_dir_on_container }}/hadoop_files/java_mappings"

    # Hadoop
    export DATA_DIR="{{ bind_dir_on_container }}/hadoop_files/data_dir"
    export HADOOP_HOME=/opt/hadoop
    export HADOOP_CONF_DIR="{{ bind_dir_on_container }}/hadoop_files/hadoop_conf"
    export HADOOP_LOG_DIR="{{ bind_dir_on_container }}/hadoop_logs"

    # YARN (if conf and log dir are undefined they will default to the hadoop ones)
    #export YARN_CONF_DIR=$HADOOP_CONF_DIR
    #export YARN_LOG_DIR=$HADOOP_LOG_DIR

    ## Hadoop and YARN users (required for hadoop 3.3.5+)
    export HDFS_NAMENODE_USER=$(whoami)
    export HDFS_DATANODE_USER=$(whoami)
    export HDFS_SECONDARYNAMENODE_USER=$(whoami)
    export YARN_RESOURCEMANAGER_USER=$(whoami)
    export YARN_NODEMANAGER_USER=$(whoami)

    # Zookeeper
    export ZOO_LOG_DIR="{{ user_home_on_container }}/zookeeper/logs"

%startscript
    ## File with the variables defined in the %environment section that will be read by the "setup.sh" script
    ENVFILE="/.singularity.d/env/90-environment.sh"

    # Run as root
    bash /opt/setup.sh
    cd /opt/BDWatchdog/MetricsFeeder && bash scripts/run_atop_stream_with_java_translation.sh

    # Run as regular user
    su - {{ user_info.user_name }} -c "source $ENVFILE \
        && bash /opt/setup.sh \
        && cd /opt/BDWatchdog/MetricsFeeder \
        && bash /opt/java_snitch.sh"
