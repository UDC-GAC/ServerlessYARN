Bootstrap: localimage
From: {{ installed_app_directory }}/../{{ app_base_type }}/{{ app_base_image_file }}

%files
    {{ installed_app_directory }}/hadoop_files /opt/hadoop_files
    {{ installed_app_directory }}/setup.sh /opt/setup.sh
    {{ installed_app_directory }}/java_snitch.sh /opt/java_snitch.sh

    # To avoid repeating hadoop download while debugging
    #"{{ installed_app_directory }}/hadoop-{{ hadoop_version }}.tar.gz" /opt/hadoop-{{ hadoop_version }}.tar.gz

%post
    # Install basic software and Java
    export DEBIAN_FRONTEND=noninteractive
    apt-get -y update
    apt-get install -y wget gettext-base openssh-client openssh-server
    apt-get install -y openjdk-8-jdk

    # Trying to setup firewall (iptables prevent hadoop cluster from working)
    #apt-get remove -y iptables
    #apt-get install -y ufw kmod
    #apt-get install -y nftables
    #apt-get install -y --reinstall linux-headers-$(uname -r)
    #apt-get -y update && apt-get -y upgrade
    #modprobe iptable_filter
    #modprobe /lib/modules/$(uname -r)/kernel/net/ipv4/netfilter/iptable_filter.ko
    #ufw --force enable

    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

    # Setup SSH
    mkdir /run/sshd
    chmod 0711 /run/sshd

    # Setup hadoop
    cd /opt
    export HADOOP_CONF_DIR=/opt/hadoop_files/hadoop_conf
    export DATA_DIR="{{ bind_dir_on_container }}/hadoop_files/data_dir"
    mkdir -p hadoop_files/data_dir

    ## Download and uncompress
    hadoop_download_dir=https://archive.apache.org/dist/hadoop/core/hadoop-{{ hadoop_version }}
    hadoop_download_file=hadoop-{{ hadoop_version }}.tar.gz
    wget $hadoop_download_dir/$hadoop_download_file
    mkdir hadoop && tar xf $hadoop_download_file -C hadoop --strip-components 1 && rm $hadoop_download_file
    chown -R $(whoami):$(whoami) hadoop

    ## Setup format script and default configuration files
    cp hadoop_files/config/format_filesystem.sh hadoop
    mkdir -p $HADOOP_CONF_DIR && cp -r hadoop/etc/hadoop/* $HADOOP_CONF_DIR/
    envsubst < hadoop_files/config/hdfs-site.xml > $HADOOP_CONF_DIR/hdfs-site.xml
    cp hadoop_files/config/core-site.xml $HADOOP_CONF_DIR/core-site.xml
    cp hadoop_files/config/mapred-site.xml $HADOOP_CONF_DIR/mapred-site.xml
    cp hadoop_files/config/yarn-site.xml $HADOOP_CONF_DIR/yarn-site.xml
    cp hadoop_files/config/workers $HADOOP_CONF_DIR/workers

    ## Set regular user as owner of required files
    chown -R {{ user_info.user_name }} hadoop_files

%environment
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    export JAVA_MAPPINGS_FOLDER_PATH="{{ bind_dir_on_container }}/hadoop_files/java_mappings"
    # Hadoop
    export DATA_DIR="{{ bind_dir_on_container }}/hadoop_files/data_dir"
    export HADOOP_HOME=/opt/hadoop
    export HADOOP_CONF_DIR="{{ bind_dir_on_container }}/hadoop_files/hadoop_conf"
    #export YARN_CONF_DIR=$HADOOP_CONF_DIR
    export HADOOP_LOG_DIR="{{ bind_dir_on_container }}/hadoop_logs"
    #export YARN_LOG_DIR=$HADOOP_LOG_DIR
    ## Hadoop users (hadoop 3.3.5)
    export HDFS_NAMENODE_USER=$(whoami)
    export HDFS_DATANODE_USER=$(whoami)
    export HDFS_SECONDARYNAMENODE_USER=$(whoami)
    export YARN_RESOURCEMANAGER_USER=$(whoami)
    export YARN_NODEMANAGER_USER=$(whoami)

%startscript
    #cd /opt/BDWatchdog/MetricsFeeder && bash scripts/run_atop_stream_tmux.sh && \
    # cd /opt/BDWatchdog/MetricsFeeder && bash scripts/run_atop_stream_with_java_translation.sh && bash /opt/java_snitch.sh && \
    # bash /opt/setup.sh
    #su - vagrant -c "cd /opt/BDWatchdog/MetricsFeeder && bash scripts/run_atop_stream_with_java_translation.sh && bash /opt/java_snitch.sh"

    ## File with the variables defined in the %environment section
    ENVFILE="/.singularity.d/env/90-environment.sh"

    # Run as root
    bash /opt/setup.sh

    # Run as regular user
    su - {{ user_info.user_name }} -c "source $ENVFILE \
        && bash /opt/setup.sh \
        && cd /opt/BDWatchdog/MetricsFeeder \
        && bash scripts/run_atop_stream_with_java_translation.sh \
        && bash /opt/java_snitch.sh"
