# Start an app on a container
- hosts: nodes
  become: no
  gather_facts: no

  vars_files:
    - vars/main.yml
    - config/config.yml

  vars:
    - jid_file: "{{ installation_path }}/async_jobs/{{ container }}.jid"
    - container_bind_dir: "{{ bind_dir }}/{{ container }}"
    - hdfs_file_list: ["core-site.xml", "hdfs-site.xml", "hadoop-env.sh", "rack_topology.py"]
    - hadoop_file_list: ["yarn-site.xml", "mapred-site.xml", "yarn-env.sh"]
    - spark_file_list: ["spark-defaults.conf"]

  environment:
    BDWATCHDOG_PATH: "{{ bdwatchdog_path }}"
    RESCALING_PATH: "{{ serverless_containers_path }}/scripts/"
    PYTHONPATH: ":{{ serverless_containers_path }}"

  ## Singularity + cgroups v1

  tasks:
  - import_tasks: tasks/utils/get_user_ids.yml
    tags: always

  ## Setup network
  ## TODO: maybe setup a DNS server is better
  ## TODO: setup iptables
  - name: Setup network on containers
    block:
      - name: Get containers IP addresses
        shell: "{% for item in containers_info %}
          {% if item.host == inventory_hostname %}
          sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} su - {{ user_info.user_name }} -c \"printf '%s ' \\$(hostname -I | awk '{print \\$1}') && hostname\" && 
          {% endif  %}
          {% endfor %}true"
        args:
          executable: /bin/bash
        register: ip_addresses

      - name: Update hostname resolution in containers
        shell: "{% for item in containers_info %}
          {% if item.host == inventory_hostname %}
          sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \
          'cp /etc/hosts {{ bind_dir_on_container }} \
          && {% for host_ip in resolution_list.split('\n') | select() %}sed -i '\\''1s/^/{{ host_ip }}\\n/'\\'' {{ bind_dir_on_container }}/hosts && {% endfor %}true \
          && cat {{ bind_dir_on_container }}/hosts > /etc/hosts \
          && rm {{ bind_dir_on_container }}/hosts' && 
          {% endif  %}
          {% endfor %}true"
        args:
          executable: /bin/bash
        vars:
          iface_ip_list: "{{ ansible_play_hosts | map('extract', hostvars, 'ip_addresses') | list }}"
          resolution_list: "{% for result in iface_ip_list | flatten(levels=1) %}{{ result.stdout }}\n{% endfor %}"

      # Iptables setup ready, but i'll leave it commented in case it interferes with hadoop/yarn tests
      # - name: Setup Iptables (1)
      #   when: item.host == inventory_hostname
      #   shell: "sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \
      #     'iptables-nft -P FORWARD DROP \
      #     && iptables-nft -P INPUT DROP \
      #     && iptables-nft -A INPUT -m state --state INVALID -j DROP \
      #     && iptables-nft -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT \
      #     && iptables-nft -A INPUT -i lo -j ACCEPT'"
      #   args:
      #     executable: /bin/bash
      #   loop: "{{ containers_info }}"

      # - name: Setup Iptables (2)
      #   when: item[0].host == inventory_hostname
      #   shell: "sudo {{ singularity_command_alias }} exec instance://{{ item[0].container_name }} bash -c \
      #     'iptables-nft -A INPUT -s {{ item[1].stdout | trim }} -j ACCEPT'"
      #   args:
      #     executable: /bin/bash
      #   vars:
      #     iface_ip_list: "{{ ansible_play_hosts | map('extract', hostvars, 'ip_addresses') | map(attribute='results') }}"
      #   loop: "{{ containers_info | product(iface_ip_list | flatten(levels=1) | reject('search','skip_reason'))|list }}"

    vars:
      - containers_info: "{{ containers_info_str | replace('\n','') | replace(' ','') }}"
    tags: never, setup_network

  - name: Setup connection on containers with global hdfs
    block:
      - name: Get global namenode IP address
        when: inventory_hostname == global_namenode_host
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ global_namenode_container }} su - {{ user_info.user_name }} -c \"printf '%s ' \\$(hostname -I | awk '{print \\$1}') && hostname\""
        args:
          executable: /bin/bash
        register: global_namenode_resolution

      - name: Update hostname resolution in containers
        when: inventory_hostname == rm_host
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ rm_container }} bash -c \
          'cp /etc/hosts {{ bind_dir_on_container }} \
          && sed -i '\\''1s/^/{{ namenode_iface_ip }}\\n/'\\'' {{ bind_dir_on_container }}/hosts \
          && cat {{ bind_dir_on_container }}/hosts > /etc/hosts \
          && rm {{ bind_dir_on_container }}/hosts'"
        args:
          executable: /bin/bash
        vars:
          namenode_iface_ip: "{{ (ansible_play_hosts | map('extract', hostvars, 'global_namenode_resolution') | first).stdout }}"
    tags: never, setup_global_hdfs_connection

  - name: Setup hadoop on containers
    block:
      - name: Workers file setup
        tags: setup_hadoop, setup_hdfs
        block:
          # - name: get the user id running the deploy
          #   become: no
          #   local_action: command id -u
          #   register: userid_on_the_host

          # - name: Copy app files already on containers & Update workers file on containers
          #   shell: "{% for item in containers_info %}
          #     {% if item.host == inventory_hostname %}
          #     sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \ 
          #     'cp -pr /opt/hadoop_files {{ bind_dir_on_container }}/ && chown -R {{ userid_on_the_host.stdout }} {{ bind_dir_on_container }}/* \
          #     && {% for item in worker_list %}echo {{ item }} >> $HADOOP_CONF_DIR/workers && {% endfor %}true' && 
          #     {% endif  %}
          #     {% endfor %}true"
          #   args:
          #     executable: /bin/bash
          #   vars:
          #     worker_list: "{{ containers_info | 
          #             map(attribute='container_name') |
          #             reject('search', rm_container) | list}}"

          - name: Copy app files already on containers & Update workers file on containers
            shell: "
              {% for item in containers_info %}
                {% if item.host == inventory_hostname %}
                  sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} su - {{ user_info.user_name }} -c '\ 
                    cp -pr /opt/hadoop_files {{ bind_dir_on_container }}/ && 
                    {% for item in worker_list %}
                      echo {{ item }} >> $HADOOP_CONF_DIR/workers && 
                    {% endfor %}true
                  ' && 
                {% endif  %}
              {% endfor %}true"
            args:
              executable: /bin/bash
            vars:
              worker_list: "{{ containers_info | 
                      map(attribute='container_name') |
                      reject('search', rm_container) | list}}"

      - name: Prepare and copy all config files to containers
        tags: setup_hadoop
        vars:
          tmpdir: "{{ installation_path }}/tmpdir_{{ app_name }}"
        block:
          - name: Create temporary directory to store templated config files
            delegate_to: localhost
            run_once: yes
            file:
              path: "{{ tmpdir }}"
              state: directory
            tags: setup_hdfs

          - name: Copy hdfs config files to master
            delegate_to: localhost
            run_once: yes
            template:
              src: "templates/hadoop/{{ item }}"
              dest: "{{ tmpdir }}/{{ item }}"
              mode: preserve
            loop: "{{ hdfs_file_list }}"
            tags: setup_hdfs

          - name: Copy hadoop config files to master
            delegate_to: localhost
            run_once: yes
            template:
              src: "templates/hadoop/{{ item }}"
              dest: "{{ tmpdir }}/{{ item }}"
            loop: "{{ hadoop_file_list }}"

          - name: Copy spark config files to master
            when: app_type is defined and app_type == 'spark_app'
            delegate_to: localhost
            run_once: yes
            template:
              src: "templates/spark/{{ item }}"
              dest: "{{ tmpdir }}/{{ item }}"
            loop: "{{ spark_file_list }}"

          - name: Copy config files to hosts
            synchronize:
              src: "{{ tmpdir }}/"
              dest: "{{ tmpdir }}/"
            tags: setup_hdfs

          - name: Copy and update hdfs config files to containers
            shell: "{% for item in containers_info %}
              {% if item.host == inventory_hostname %}
              {% for file in hdfs_file_list %}
              {% set bind_dir = item.disk_path + '/' + bind_dir_name if item.disk_path is defined else default_bind_path + '/' + bind_dir_name %}
              cp {{ tmpdir }}/{{ file }} {{ bind_dir }}/{{ item.container_name }}/{{ file }} && 
              {% endfor %}
              sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} su - {{ user_info.user_name }} -c \
              '{% for file in hdfs_file_list %}cp {{ bind_dir_on_container }}/{{ file }} $HADOOP_CONF_DIR/{{ file }} && {% endfor %}true' && 
              {% endif  %}
              {% endfor %}true"
            args:
              executable: /bin/bash
            tags: setup_hdfs

          - name: Copy and update hadoop config files to containers
            shell: "{% for item in containers_info %}
              {% if item.host == inventory_hostname %}
              {% for file in hadoop_file_list %}
              {% set bind_dir = item.disk_path + '/' + bind_dir_name if item.disk_path is defined else default_bind_path + '/' + bind_dir_name %}
              cp {{ tmpdir }}/{{ file }} {{ bind_dir }}/{{ item.container_name }}/{{ file }} && 
              {% endfor %}
              sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} su - {{ user_info.user_name }} -c \
              '{% for file in hadoop_file_list %}cp {{ bind_dir_on_container }}/{{ file }} $HADOOP_CONF_DIR/{{ file }} && {% endfor %}true' && 
              {% endif  %}
              {% endfor %}true"
            args:
              executable: /bin/bash

          - name: Copy and update spark config files to containers
            when: app_type is defined and app_type == 'spark_app'
            shell: "{% for item in containers_info %}
              {% if item.host == inventory_hostname %}
              {% for file in spark_file_list %}
              {% set bind_dir = item.disk_path + '/' + bind_dir_name if item.disk_path is defined else default_bind_path + '/' + bind_dir_name %}
              cp {{ tmpdir }}/{{ file }} {{ bind_dir }}/{{ item.container_name }}/{{ file }} && 
              {% endfor %}
              sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} su - {{ user_info.user_name }} -c \
              '{% for file in spark_file_list %}cp {{ bind_dir_on_container }}/{{ file }} $SPARK_CONF_DIR/{{ file }} && {% endfor %}true' && 
              {% endif  %}
              {% endfor %}true"
            args:
              executable: /bin/bash

          - name: Remove temporary directory in server and hosts
            file:
              path: "{{ tmpdir }}"
              state: absent
            delegate_to: "{{ item }}"
            run_once: yes
            with_items:
              - localhost
              - "{{ ansible_play_hosts }}"
            tags: setup_hdfs

      - name: Setup SSH connection
        tags: setup_hadoop, setup_hdfs
        block:
          - name: Update ssh known_hosts on ResourceManager/Namenode container & Move RM/NN container ssh public key
            when: rm_host == inventory_hostname
            shell: "sudo {{ singularity_command_alias }} exec instance://{{ rm_container }} su - {{ user_info.user_name }} -c \
              'ssh-keyscan -t rsa `hostname` >> ~/.ssh/known_hosts \
              && ssh-keyscan -t rsa -f $HADOOP_CONF_DIR/workers >> ~/.ssh/known_hosts \
              && cp ~/.ssh/id_rsa.pub {{ bind_dir_on_container }}/{{ rm_container }}.pub'"
            args:
              executable: /bin/bash

          ## TODO: fusion with fetch below
          - name: Prepare RM/NN container bind path variable
            when: "item.container_name == rm_container"
            set_fact:
              rm_container_bind_path: "{{ item.disk_path if item.disk_path is defined else default_bind_path }}"
            with_items:
            - "{{ containers_info }}"

          # - name: Create app directory on server for ssh public key
          #   delegate_to: localhost
          #   run_once: yes
          #   file:
          #     path: "{{ installation_path }}/apps/{{ app_name }}/"
          #     state: directory

          # - name: Fetch RM/NN container ssh public key
          #   when: rm_host == inventory_hostname
          #   synchronize:
          #     src: "{{ bind_dir }}/{{ rm_container }}/{{ rm_container }}.pub"
          #     dest: "{{ installation_path }}/apps/{{ app_name }}/{{ rm_container }}.pub"
          #     mode: pull
          #   vars:
          #     bind_path: "{{ rm_container_bind_path }}"

          - name: Create temporary app directory on hosts for ssh public key
            file:
              path: "{{ installation_path }}/app_keys/{{ app_name }}/"
              state: directory

          # - name: Copy RM/NN container ssh public key to hosts
          #   synchronize:
          #     src: "{{ installation_path }}/apps/{{ app_name }}/{{ rm_container }}.pub"
          #     dest: "{{ installation_path }}/apps/{{ app_name }}/{{ rm_container }}.pub"

          - name: Copy RM/NN container ssh public key to hosts
            delegate_to: "{{ rm_host }}" # this will push from rm_host to each host, i.e., a remote host to remote host copy
            synchronize:
              src: "{{ bind_dir }}/{{ rm_container }}/{{ rm_container }}.pub"
              dest: "{{ installation_path }}/app_keys/{{ app_name }}/{{ rm_container }}.pub"
            vars:
              bind_path: "{{ rm_container_bind_path }}"

          - name: Copy and append RM/NN container ssh public key to the other containers
            shell: "{% for item in containers_info %}
              {% if item.host == inventory_hostname and item.container_name != rm_container %}
              {% set bind_dir = item.disk_path + '/' + bind_dir_name if item.disk_path is defined else default_bind_path + '/' + bind_dir_name %}
              cp {{ installation_path }}/app_keys/{{ app_name }}/{{ rm_container }}.pub {{ bind_dir }}/{{ item.container_name }}/{{ rm_container }}.pub && \
              sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} su - {{ user_info.user_name }} -c \
              'cat {{ bind_dir_on_container }}/{{ rm_container }}.pub >> ~/.ssh/authorized_keys \
              && rm {{ bind_dir_on_container }}/{{ rm_container }}.pub' && 
              {% endif  %}
              {% endfor %}true"
            args:
              executable: /bin/bash

          - name: Remove temporary directory with SSH keys on host
            file:
              path: "{{ installation_path }}/app_keys/{{ app_name }}/"
              state: absent

      - name: Start HDFS + YARN
        tags: setup_hadoop
        block:
          - name: Format filesystem, start HDFS and YARN (Master)
            when: rm_host == inventory_hostname
            shell: "sudo {{ singularity_command_alias }} exec instance://{{ rm_container }} su - {{ user_info.user_name }} -c \
              '(cd $HADOOP_HOME \
              && bash format_filesystem.sh) \
              && (nohup $HADOOP_HOME/bin/hdfs --daemon start namenode & \
              nohup $HADOOP_HOME/bin/yarn --daemon start resourcemanager &)'"
            args:
              executable: /bin/bash

          - name: Start HDFS and YARN (Worker)
            shell: "{% for item in containers_info %}
              {% if item.host == inventory_hostname and item.container_name != rm_container %}
              sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} su - {{ user_info.user_name }} -c \
              'nohup $HADOOP_HOME/bin/hdfs --daemon start datanode & \
              nohup $HADOOP_HOME/bin/yarn --daemon start nodemanager &' && 
              {% endif  %}
              {% endfor %}true"
            args:
              executable: /bin/bash

      - name: Start only HDFS
        tags: setup_hdfs
        block:
          - name: Format filesystem and start HDFS (Master)
            when: rm_host == inventory_hostname
            shell: "sudo {{ singularity_command_alias }} exec instance://{{ rm_container }} su - {{ user_info.user_name }} -c \
              '(cd $HADOOP_HOME \
              && bash format_filesystem.sh) \
              && (nohup $HADOOP_HOME/bin/hdfs --daemon start namenode &)'"
            args:
              executable: /bin/bash

          - name: Start HDFS (Worker)
            shell: "{% for item in containers_info %}
              {% if item.host == inventory_hostname and item.container_name != rm_container %}
              sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} su - {{ user_info.user_name }} -c \
              'nohup $HADOOP_HOME/bin/hdfs --daemon start datanode &' && 
              {% endif  %}
              {% endfor %}true"
            args:
              executable: /bin/bash

      - name: Wait some seconds for Namenode to exit safe mode
        tags: setup_hadoop, setup_hdfs
        pause:
          seconds: 10
    vars:
      - containers_info: "{{ containers_info_str | replace('\n','') | replace(' ','') }}"
    tags: never

  ## Copy app files
  - name: Copy app files to container
    block:

      - name: Copy app files already on container if there was an installation
        when: "install_script != ''"
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} su - {{ user_info.user_name }} -c 'cp -pr /opt/{{ app_dir | basename }}/* {{ bind_dir_on_container }}'"
        args:
          executable: /bin/bash

      - name: Create temporary directory to store container files
        delegate_to: localhost
        file:
          path: "/tmp/cont-config/{{ container }}"
          state: directory
          owner: "{{ user_info.user_name }}"

      - block:
          - name: Create app files subdirectory in container
            file:
              path: "{{ container_bind_dir }}/{{ runtime_files | basename }}"
              state: directory

          - name: Create app files subdirectory in localhost (temporary dir)
            delegate_to: localhost
            file:
              path: "/tmp/cont-config/{{ container }}/{{ runtime_files | basename }}"
              state: directory
              owner: "{{ user_info.user_name }}"

          - name: Find config files to template in the app directory
            delegate_to: localhost
            run_once: yes
            find:
              paths: "apps/{{ app_dir }}/{{ runtime_files }}/"
              recurse: yes
            register: found_config_files

          - name: Template container config files and store in temporary directory
            delegate_to: localhost
            template:
              src: "{{ item.path }}"
              dest: "/tmp/cont-config/{{ container }}/{{ runtime_files | basename }}/{{ item.path | basename }}"
            loop: "{{ found_config_files.files }}"
        when: "runtime_files != ''"

      - name: Create output directory in container
        when: output_dir is defined and output_dir != ""
        file:
          path: "{{ container_bind_dir }}/{{ output_dir | basename }}"
          state: directory

    tags: never, start_app, stop_app

    ## Start
  - name: Start app on container
    block:

      - name: Copy app start script
        delegate_to: localhost
        template:
          src: "apps/{{ app_dir }}/{{ start_script }}"
          dest: "/tmp/cont-config/{{ container }}/"
          mode: preserve

      - name: Copy app jar
        delegate_to: localhost
        when: "app_jar != ''"
        copy:
          src: "apps/{{ app_dir }}/{{ app_jar }}"
          dest: "/tmp/cont-config/{{ container }}/"

      - name: Copy templated files to remote container
        copy:
          src: "/tmp/cont-config/{{ container }}/"
          dest: "{{ container_bind_dir }}/"
          owner: "{{ user_info.user_name }}"

      - name: Execute start script on container
        #shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} nohup bash {{ bind_dir_on_container }}/{{ start_script | basename }} </dev/null"
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} su - {{ user_info.user_name }} -c 'nohup bash {{ bind_dir_on_container }}/{{ start_script | basename }} </dev/null'"
        args:
          executable: /bin/bash
        async: 3600
        poll: 0
        register: start_job

      - name: Save job id to check status later
        delegate_to: localhost
        copy:
          content: "{{ start_job.ansible_job_id }}"
          dest: "{{ jid_file }}"
          owner: "{{ user_info.user_name }}"

    tags: never, start_app

  ## Wait
  - name: Wait for app container
    block:
      - name: Read start script job id
        set_fact:
          start_job_id: "{{ lookup('file', jid_file) | trim }}"

      - name: Wait until start script has finished
        async_status:
          jid: "{{ start_job_id}}"
        register: start_status
        until: start_status.finished
        retries: 120
        delay: 30

    tags: never, wait_app

  ## Stop
  - name: Stop app on container
    block:
      # Run stop script if defined
      - block:
        - name: Copy app stop script
          delegate_to: localhost
          template:
            src: "apps/{{ app_dir }}/{{ stop_script }}"
            dest: "/tmp/cont-config/{{ container }}/"
            mode: preserve

        - name: Copy templated files to remote container
          copy:
            src: "/tmp/cont-config/{{ container }}/"
            dest: "{{ container_bind_dir }}/"
            owner: "{{ user_info.user_name }}"

        - name: Execute stop script on container
          #shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} nohup bash {{ bind_dir_on_container }}/{{ stop_script | basename }} </dev/null"
          shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} su - {{ user_info.user_name }} -c 'nohup bash {{ bind_dir_on_container }}/{{ stop_script | basename }} </dev/null'"
          args:
            executable: /bin/bash
          async: 3600
          poll: 0
          register: stop_job

        - name: Wait some seconds before polling
          pause:
            seconds: 10

        - name: Wait until stop script has finished
          async_status:
            jid: "{{ stop_job.ansible_job_id }}"
          register: stop_status
          until: stop_status.finished
          retries: 120
          delay: 30

        when: stop_script is defined and stop_script != ""

      # Stop java snitcher
      - name: Stop java snitcher
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} su - {{ user_info.user_name }} -c \
          'tmux kill-session -t JAVA_SNITCH'"
        args:
          executable: /bin/bash
        ignore_errors: yes

      # - name: Change remaining files permissions (hadoop)
      #   # Check empty string to avoid unwanted problems if the variable is somehow wrongly defined
      #   when: "app_jar != '' and ' ' not in userid_on_the_host.stdout"
      #   shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} bash -c \
      #     'cd {{ bind_dir_on_container }} \
      #     && chown -R {{ userid_on_the_host.stdout }} hadoop_logs'"
      #   args:
      #     executable: /bin/bash

      # # TODO: actually transfer files from nodes to server, instead of relying on shared folders (NFS and such)
      # - name: Copy hadoop logs to output_dir
      #   when: "app_jar != '' and input_log_dir != output_log_dir"
      #   copy:
      #     src: "{{ input_log_dir }}"
      #     dest: "{{ output_log_dir }}"
      #     remote_src: yes
      #   vars:
      #     input_log_dir: "{{ container_bind_dir }}/hadoop_logs/"
      #     output_log_dir: "{{ default_bind_path }}/{{ bind_dir_name }}/{{ rm_container }}/hadoop_logs/"

      ## Manage output data
      - block:
        - name: Set output data timestamp
          when: timestamp is not defined # --> only set current timestamp if another timestamp has not been set
          set_fact:
            timestamp: "{{ now(fmt='%Y-%m-%d--%H-%M-%S') }}"

        - name: Move hadoop logs to output_dir
          when: "app_jar is defined and app_jar != ''"
          shell: "mv {{ container_bind_dir }}/hadoop_logs {{ container_bind_dir }}/{{ output_dir }}/hadoop_logs"
          args:
            executable: /bin/bash

        - block:
          - name: Create output directory on server
            delegate_to: localhost
            file:
              path: "{{ stored_output_dir }}"
              state: directory

          - name: Fetch output data from container to server
            vars:
              ansible_ssh_extra_args: "-o ControlPath=~/.ansible/cp/socket-{{ inventory_hostname }}"
            synchronize:
              src: "{{ container_bind_dir }}/{{ output_dir }}/"
              dest: "{{ stored_output_dir }}"
              mode: pull
              ssh_connection_multiplexing: yes
              use_ssh_args: yes
          vars:
            stored_output_dir: "{{ app_output_data }}/{{ app_name }}/{{ timestamp }}/{{ output_dir }}-{{ container }}"
        when: "output_dir is defined and output_dir != ''"

      # ## Clean hadoop apps
      # - block:
      #   - name: Remove hadoop config files
      #     file:
      #       path: "{{ container_bind_dir }}/{{ item }}"
      #       state: absent
      #     loop: "{{ hdfs_file_list + hadoop_file_list + spark_file_list }}"

      #   - name: Remove hadoop local logs
      #     when: "input_log_dir != output_log_dir"
      #     file:
      #       path: "{{ input_log_dir }}"
      #       state: absent
      #     vars:
      #       input_log_dir: "{{ container_bind_dir }}/hadoop_logs"
      #       output_log_dir: "{{ default_bind_path }}/{{ bind_dir_name }}/{{ rm_container }}/hadoop_logs"

      #   - name: Remove hadoop files directory in container
      #     # Check empty string to avoid unwanted problems if the variable is somehow wrongly defined
      #     when: "' ' not in bind_dir_on_container"
      #     shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} bash -c \
      #       'rm -r {{ bind_dir_on_container}}/hadoop_files'"
      #     args:
      #       executable: /bin/bash
  
      # when: "app_jar is defined and app_jar != ''"

    tags: never, stop_app

  ## Stop hadoop
  - name: Stop hadoop cluster
    block:
      - name: Stop hadoop cluster
        when: rm_host == inventory_hostname
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ rm_container }} su - {{ user_info.user_name }} -c \
          'cd $HADOOP_HOME \
          && sbin/stop-yarn.sh \
          && sbin/stop-dfs.sh'"
        args:
          executable: /bin/bash
    tags: never, stop_hadoop_cluster

  # ## Clean HDFS files
  # - name: Workaround to clean global HDFS files
  #   when: "' ' not in bind_dir_on_container"
  #   shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} bash -c \
  #     'tmux kill-session -t JAVA_SNITCH && rm -r {{ bind_dir_on_container}}/hadoop_files && rm -r {{ bind_dir_on_container}}/hadoop_logs'"
  #   args:
  #     executable: /bin/bash
  #   tags: never, clean_hdfs

  - name: Stop Java Snitch
    shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} su - {{ user_info.user_name }} -c \
      'tmux kill-session -t JAVA_SNITCH'"
    args:
      executable: /bin/bash
    tags: never, clean_hdfs

  # # TODO: do not rely on shared folders (NFS and such) to access hadoop_logs
  # ## Set hadoop logs timestamp
  # - name: Set hadoop logs timestamp
  #   delegate_to: localhost
  #   run_once: yes
  #   shell: "mv hadoop_logs hadoop_logs_`date +%d-%m-%y--%H-%M-%S`"
  #   args:
  #     chdir: "{{ default_bind_path }}/{{ bind_dir_name }}/{{ rm_container }}"
  #     executable: /bin/bash
  #   tags: never, set_hadoop_logs_timestamp

  ## TODO: Remove app_jar conditions in these tasks (in the whole file)

  ## HDFS
  - name: Create directory on HDFS
    when: dest_path is defined and dest_path != ''
    shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} su - {{ user_info.user_name }} -c '$HADOOP_HOME/bin/hdfs dfs -mkdir -p {{ dest_path }}'"
    args:
      executable: /bin/bash
    tags: never, create_dir_on_hdfs

  - name: Remove file from HDFS
    when: dest_path is defined and dest_path != ''
    shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} su - {{ user_info.user_name }} -c '$HADOOP_HOME/bin/hdfs dfs -rm -r {{ dest_path }}'"
    args:
      executable: /bin/bash
    tags: never, remove_file_from_hdfs

  - name: Upload file(s) to HDFS
    tags: never, add_file_to_hdfs
    when: dest_path is defined and dest_path != '' and origin_path is defined and origin_path != ''
    delegate_to: localhost
    vars:
      global_namenode_url: "hdfs://{{ namenode_container }}:8020"
      origin_container_path: "{{ bind_dir_on_container }}/{{ origin_path.rstrip('/') | basename }}"
    block:
      - name: Put data on HDFS
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ hdfs_frontend_container_name }} su - {{ user_info.user_name }} -c \
          '$HADOOP_HOME/bin/hdfs dfs -put {{ origin_container_path }} {{ global_namenode_url }}/{{ dest_path }}'"
        args:
          executable: /bin/bash

  - name: Download file(s) from HDFS
    tags: never, get_file_from_hdfs
    when: dest_path is defined and origin_path is defined and origin_path != ''
    delegate_to: localhost
    vars:
      global_namenode_url: "hdfs://{{ namenode_container }}:8020"
      dest_container_path: "{{ bind_dir_on_container }}/{{ dest_path.rstrip('/') }}" 
    block:
      - name: Get data from HDFS
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ hdfs_frontend_container_name }} su - {{ user_info.user_name }} -c \ 
          '$HADOOP_HOME/bin/hdfs dfs -get {{ global_namenode_url }}/{{ origin_path }} {{ dest_container_path }}'"
        args:
          executable: /bin/bash

  - name: Move data between global and local HDFS
    tags: never
    when: inventory_hostname == rm_host
    vars:
      global_namenode_ip: "\\$(getent hosts {{ global_namenode_container }} | awk '{print \\$1 }')" ## for some reason distcp does not resolve hostnames, so this workaround is needed to resolve ip before executing distcp
      global_namenode_url: "hdfs://{{ global_namenode_ip }}:8020"
    block:
      - name: Download data from global HDFS to local one
        tags: download_to_local
        when: global_input is defined and global_input != '' and local_output is defined and local_output != ''
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ rm_container }} su - {{ user_info.user_name }} -c \"\\$HADOOP_HOME/bin/hadoop distcp {{ origin }} {{ dest }}\""
        args:
          executable: /bin/bash
        vars:
          origin: "{{ global_namenode_url }}/{{ global_input }}"
          dest: "{{ local_output }}/{{ global_input | basename }}"

      - name: Upload data from local HDFS to global one
        tags: upload_to_global
        when: local_input is defined and local_input != '' and global_output is defined and global_output != ''
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ rm_container }} su - {{ user_info.user_name }} -c \"\\$HADOOP_HOME/bin/hadoop distcp {{ origin }} {{ dest }}\""
        args:
          executable: /bin/bash
        vars:
          origin: "{{ local_input }}"
          dest: "{{ global_namenode_url }}/{{ global_output }}/{{ local_input | basename }}"
