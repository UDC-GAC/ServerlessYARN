# Start an app on a container
- hosts: nodes
  become: no
  gather_facts: no

  vars_files:
    - vars/main.yml
    - config/config.yml

  vars:
    - container_bind_dir: "{{ bind_dir }}/{{ container }}"

  environment:
    BDWATCHDOG_PATH: "{{ bdwatchdog_path }}"
    RESCALING_PATH: "{{ serverless_containers_path }}/scripts/"
    PYTHONPATH: ":{{ serverless_containers_path }}"

  ## Singularity + cgroups v1
  tasks:
  ## Setup network
  ## TODO: maybe setup a DNS server is better
  ## TODO: setup iptables
  - name: Setup network on containers
    block:
      - name: Get containers IP addresses
        shell: "{% for item in containers_info %}
          {% if item.host == inventory_hostname %}
          sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \"printf '%s ' \\$(hostname -I | awk '{print \\$1}') && hostname\" && 
          {% endif  %}
          {% endfor %}true"
        args:
          executable: /bin/bash
        register: ip_addresses

      - name: Update hostname resolution in containers
        shell: "{% for item in containers_info %}
          {% if item.host == inventory_hostname %}
          sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \
          'cp /etc/hosts {{ bind_dir_on_container }} \
          && {% for host_ip in resolution_list.split('\n') | select() %}sed -i '\\''1s/^/{{ host_ip }}\\n/'\\'' {{ bind_dir_on_container }}/hosts && {% endfor %}true \
          && cat {{ bind_dir_on_container }}/hosts > /etc/hosts \
          && rm {{ bind_dir_on_container }}/hosts' && 
          {% endif  %}
          {% endfor %}true"
        args:
          executable: /bin/bash
        vars:
          iface_ip_list: "{{ ansible_play_hosts | map('extract', hostvars, 'ip_addresses') | list }}"
          resolution_list: "{% for result in iface_ip_list | flatten(levels=1) %}{{ result.stdout }}\n{% endfor %}"

      # Iptables setup ready, but i'll leave it commented in case it interferes with hadoop/yarn tests
      # - name: Setup Iptables (1)
      #   when: item.host == inventory_hostname
      #   shell: "sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \
      #     'iptables-nft -P FORWARD DROP \
      #     && iptables-nft -P INPUT DROP \
      #     && iptables-nft -A INPUT -m state --state INVALID -j DROP \
      #     && iptables-nft -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT \
      #     && iptables-nft -A INPUT -i lo -j ACCEPT'"
      #   args:
      #     executable: /bin/bash
      #   loop: "{{ containers_info }}"

      # - name: Setup Iptables (2)
      #   when: item[0].host == inventory_hostname
      #   shell: "sudo {{ singularity_command_alias }} exec instance://{{ item[0].container_name }} bash -c \
      #     'iptables-nft -A INPUT -s {{ item[1].stdout | trim }} -j ACCEPT'"
      #   args:
      #     executable: /bin/bash
      #   vars:
      #     iface_ip_list: "{{ ansible_play_hosts | map('extract', hostvars, 'ip_addresses') | map(attribute='results') }}"
      #   loop: "{{ containers_info | product(iface_ip_list | flatten(levels=1) | reject('search','skip_reason'))|list }}"

    vars:
      - containers_info: "{{ containers_info_str | replace('\n','') | replace(' ','') }}"

    tags: never, setup_network

  - name: Setup hadoop on containers
    block:
      - name: get the user id running the deploy
        become: no
        local_action: command id -u
        register: userid_on_the_host

      - name: Copy app files already on containers & Update workers file on containers
        shell: "{% for item in containers_info %}
          {% if item.host == inventory_hostname %}
          sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \ 
          'cp -pr /opt/files_dir {{ bind_dir_on_container }}/ && chown -R {{ userid_on_the_host.stdout }} {{ bind_dir_on_container }}/* \
          && {% for item in worker_list %}echo {{ item }} >> $HADOOP_CONF_DIR/workers && {% endfor %}true' && 
          {% endif  %}
          {% endfor %}true"
        args:
          executable: /bin/bash
        vars:
          worker_list: "{{ containers_info | 
                  map(attribute='container_name') |
                  reject('search', rm_container) | list}}"

      - name: Prepare and copy all config files to containers
        block:
          - name: Copy hadoop config files to master
            delegate_to: localhost
            run_once: yes
            template:
              src: "templates/hadoop/{{ item }}"
              dest: "{{ tmpdir }}/{{ item }}"
            loop: "{{ hadoop_file_list }}"

          - name: Copy spark config files to master
            delegate_to: localhost
            run_once: yes
            template:
              src: "templates/spark/{{ item }}"
              dest: "{{ tmpdir }}/{{ item }}"
            loop: "{{ spark_file_list }}"

          - name: Copy config files to hosts
            synchronize:
              src: "{{ tmpdir }}/"
              dest: "{{ tmpdir }}/"

          - name: Copy and update hadoop config files to containers
            shell: "{% for item in containers_info %}
              {% if item.host == inventory_hostname %}
              {% for file in hadoop_file_list %}
              {% set bind_dir = item.disk_path + '/' + bind_dir_name if item.disk_path is defined else default_bind_path + '/' + bind_dir_name %}
              cp {{ tmpdir }}/{{ file }} {{ bind_dir }}/{{ item.container_name }}/{{ file }} && 
              {% endfor %}
              sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \
              '{% for file in hadoop_file_list %}cp {{ bind_dir_on_container }}/{{ file }} $HADOOP_CONF_DIR/{{ file }} && {% endfor %}true' && 
              {% endif  %}
              {% endfor %}true"
            args:
              executable: /bin/bash

          - name: Copy and update spark config files to containers
            shell: "{% for item in containers_info %}
              {% if item.host == inventory_hostname %}
              {% for file in spark_file_list %}
              {% set bind_dir = item.disk_path + '/' + bind_dir_name if item.disk_path is defined else default_bind_path + '/' + bind_dir_name %}
              cp {{ tmpdir }}/{{ file }} {{ bind_dir }}/{{ item.container_name }}/{{ file }} && 
              {% endfor %}
              sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \
              '{% for file in spark_file_list %}cp {{ bind_dir_on_container }}/{{ file }} $SPARK_CONF_DIR/{{ file }} && {% endfor %}true' && 
              {% endif  %}
              {% endfor %}true"
            args:
              executable: /bin/bash

        vars:
          hadoop_file_list: ["core-site.xml", "hdfs-site.xml", "yarn-site.xml", "mapred-site.xml", "hadoop-env.sh", "yarn-env.sh"]
          spark_file_list: ["spark-defaults.conf"]

      - name: Update ssh known_hosts on ResourceManager/Namenode container & Move RM/NN container ssh public key
        when: rm_host == inventory_hostname
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ rm_container }} bash -c \
          'ssh-keyscan -t rsa `hostname` >> ~/.ssh/known_hosts \
          && ssh-keyscan -t rsa -f $HADOOP_CONF_DIR/workers >> ~/.ssh/known_hosts \
          && cp ~/.ssh/id_rsa.pub {{ bind_dir_on_container }}/{{ rm_container }}.pub'"
        args:
          executable: /bin/bash

      ## TODO: fusion with fetch below
      - name: Prepare RM/NN container bind path variable
        when: "item.container_name == rm_container"
        set_fact:
          rm_container_bind_path: "{{ item.disk_path if item.disk_path is defined else default_bind_path }}"
        with_items:
        - "{{ containers_info }}"

      - name: Create app directory on server for ssh public key
        delegate_to: localhost
        run_once: yes
        file:
          path: "{{ installation_path }}/apps/{{ app_name }}/"
          state: directory

      - name: Fetch RM/NN container ssh public key
        when: rm_host == inventory_hostname
        synchronize:
          src: "{{ bind_dir }}/{{ rm_container }}/{{ rm_container }}.pub"
          dest: "{{ installation_path }}/apps/{{ app_name }}/{{ rm_container }}.pub"
          mode: pull
        vars:
          bind_path: "{{ rm_container_bind_path }}"

      - name: Create app directory on hosts for ssh public key
        file:
          path: "{{ installation_path }}/apps/{{ app_name }}/"
          state: directory

      - name: Copy RM/NN container ssh public key to hosts
        synchronize:
          src: "{{ installation_path }}/apps/{{ app_name }}/{{ rm_container }}.pub"
          dest: "{{ installation_path }}/apps/{{ app_name }}/{{ rm_container }}.pub"

      - name: Copy and append RM/NN container ssh public key to the other containers
        shell: "{% for item in containers_info %}
          {% if item.host == inventory_hostname and item.container_name != rm_container %}
          {% set bind_dir = item.disk_path + '/' + bind_dir_name if item.disk_path is defined else default_bind_path + '/' + bind_dir_name %}
          cp {{ installation_path }}/apps/{{ app_name }}/{{ rm_container }}.pub {{ bind_dir }}/{{ item.container_name }}/{{ rm_container }}.pub && \
          sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \
          'cat {{ bind_dir_on_container }}/{{ rm_container }}.pub >> ~/.ssh/authorized_keys \
          && rm {{ bind_dir_on_container }}/{{ rm_container }}.pub' && 
          {% endif  %}
          {% endfor %}true"
        args:
          executable: /bin/bash

      - name: Format filesystem, start HDFS and YARN (Master)
        when: rm_host == inventory_hostname
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ rm_container }} bash -c \
          '(cd $HADOOP_HOME \
          && bash format_filesystem.sh) \
          && (nohup $HADOOP_HOME/bin/hdfs --daemon start namenode & \
          nohup $HADOOP_HOME/bin/yarn --daemon start resourcemanager &)'"
        args:
          executable: /bin/bash

      - name: Start HDFS and YARN (Worker)
        shell: "{% for item in containers_info %}
          {% if item.host == inventory_hostname and item.container_name != rm_container %}
          sudo {{ singularity_command_alias }} exec instance://{{ item.container_name }} bash -c \
          'nohup $HADOOP_HOME/bin/hdfs --daemon start datanode & \
          nohup $HADOOP_HOME/bin/yarn --daemon start nodemanager &' && 
          {% endif  %}
          {% endfor %}true"
        args:
          executable: /bin/bash

      - name: Wait some seconds for Namenode to exit safe mode
        pause:
          seconds: 10

    vars:
      - containers_info: "{{ containers_info_str | replace('\n','') | replace(' ','') }}"
    tags: never, setup_hadoop

  ## Start
  - name: Start app on container
    block:
      - name: get the user id running the deploy
        become: no
        local_action: command id -u
        register: userid_on_the_host

      - name: Copy app files already on container if there was an installation
        when: "install_script != ''"
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} bash -c 'cp -pr /opt/{{ app_dir }}/* {{ bind_dir_on_container }} && chown -R {{ userid_on_the_host.stdout }} {{ bind_dir_on_container }}/*'"
        args:
          executable: /bin/bash

      # - name: Check if app files directory is already there
      #   when: "files_dir != ''"
      #   stat:
      #     path: "{{ container_bind_dir }}/{{ files_dir | basename }}"
      #     get_checksum: false
      #     get_mime: false
      #     get_attributes: false
      #   register: stat_output
      #   tags: stop_app

      - name: Create app files directory
        when: "files_dir != ''"
        file:
          path: "{{ container_bind_dir }}/{{ files_dir | basename }}"
          state: directory
        tags: stop_app

      - name: Copy app files
        when: "files_dir != ''"
        synchronize:
          src: "apps/{{ app_dir }}/{{ files_dir }}/"
          dest: "{{ container_bind_dir }}/{{ files_dir | basename }}/"
          recursive: true
          perms: true
        tags: stop_app

      - name: Copy app start script
        template:
          src: "apps/{{ app_dir }}/{{ start_script }}"
          dest: "{{ container_bind_dir }}/"
          mode: preserve

      - name: Copy app stop script
        template:
          src: "apps/{{ app_dir }}/{{ stop_script }}"
          dest: "{{ container_bind_dir }}/"
          mode: preserve
        tags: stop_app

      - name: Copy app jar
        when: "app_jar != ''"
        copy:
          src: "apps/{{ app_dir }}/{{ app_jar }}"
          dest: "{{ container_bind_dir }}/"

      - name: Execute start script on container
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} nohup bash {{ bind_dir_on_container }}/{{ start_script | basename }} </dev/null"
        args:
          executable: /bin/bash

    tags: never, start_app

  ## Stop
  - name: Stop app on container
    block:
      - name: get the user id running the deploy
        become: no
        local_action: command id -u
        register: userid_on_the_host

      - name: Execute stop script on container
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} nohup bash {{ bind_dir_on_container }}/{{ stop_script | basename }} </dev/null"
        args:
          executable: /bin/bash

      - name: Remove app install script
        when: "install_script != ''"
        file:
          path: "{{ container_bind_dir }}/{{ install_script | basename }}"
          state: absent

      - name: Remove app start script
        file:
          path: "{{ container_bind_dir }}/{{ start_script | basename }}"
          state: absent

      - name: Remove app stop script
        file:
          path: "{{ container_bind_dir }}/{{ stop_script | basename }}"
          state: absent

      - name: Remove app jar
        when: "app_jar != ''"
        file:
          path: "{{ container_bind_dir }}/{{ app_jar | basename }}"
          state: absent

      - name: Remove hadoop config files
        when: "app_jar != ''"
        file:
          path: "{{ container_bind_dir }}/{{ item }}"
          state: absent
        vars:
          file_list: ["core-site.xml", "hdfs-site.xml", "yarn-site.xml", "mapred-site.xml", "hadoop-env.sh", "yarn-env.sh", "spark-defaults.conf"]
        loop: "{{ file_list }}"

      - name: Remove container own ssh key
        file:
          path: "{{ container_bind_dir }}/{{ container }}.pub"
          state: absent

      # Stop java snitcher
      - name: Stop java snitcher
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} bash -c \
          'tmux kill-session -t JAVA_SNITCH'"
        args:
          executable: /bin/bash
        ignore_errors: yes

      - name: Remove hadoop files directory in container
        # Check empty string to avoid unwanted problems if the variable is somehow wrongly defined
        when: "app_jar != '' and ' ' not in bind_dir_on_container"
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} bash -c \
          'rm -r {{ bind_dir_on_container}}/files_dir'"
        args:
          executable: /bin/bash

      # Workaround to avoid having result files root-owned
      # TODO: move results to another directory accesible for the user
      - name: Change remaining files permissions
        # Check empty string to avoid unwanted problems if the variable is somehow wrongly defined
        when: "' ' not in userid_on_the_host.stdout"
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} bash -c \
          'cd {{ bind_dir_on_container }} \
          && chown -R {{ userid_on_the_host.stdout }} output_* | : \
          && chown {{ userid_on_the_host.stdout }} runtime_* | : \
          && chown {{ userid_on_the_host.stdout }} app_log_* | : \
          && chown {{ userid_on_the_host.stdout }} datagen_log_* | :'"
        args:
          executable: /bin/bash

      - name: Change remaining files permissions (hadoop)
        # Check empty string to avoid unwanted problems if the variable is somehow wrongly defined
        when: "app_jar != '' and ' ' not in userid_on_the_host.stdout"
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ container }} bash -c \
          'cd {{ bind_dir_on_container }} \
          && chown -R {{ userid_on_the_host.stdout }} hadoop_logs'"
        args:
          executable: /bin/bash

      # TODO: actually transfer files from nodes to server, instead of relying on shared folders (NFS and such)
      - name: Copy hadoop logs to server node
        when: "app_jar != '' and input_dir != output_dir"
        copy:
          src: "{{ input_dir }}"
          dest: "{{ output_dir }}"
          remote_src: yes
        vars:
          input_dir: "{{ container_bind_dir }}/hadoop_logs/"
          output_dir: "{{ default_bind_path }}/{{ bind_dir_name }}/{{ rm_container }}/hadoop_logs/"

      - name: Remove hadoop local logs
        when: "app_jar != '' and input_dir != output_dir"
        file:
          path: "{{ input_dir }}"
          state: absent
        vars:
          input_dir: "{{ container_bind_dir }}/hadoop_logs"
          output_dir: "{{ default_bind_path }}/{{ bind_dir_name }}/{{ rm_container }}/hadoop_logs"

      - name: Remove app files directory
        when: "files_dir != ''"
        file:
          path: "{{ container_bind_dir }}/{{ files_dir | basename }}"
          state: absent

    tags: never, stop_app

  ## Stop hadoop
  - name: Stop hadoop cluster
    block:
      - name: Stop hadoop cluster
        when: rm_host == inventory_hostname
        shell: "sudo {{ singularity_command_alias }} exec instance://{{ rm_container }} bash -c \
          'cd $HADOOP_HOME \
          && sbin/stop-yarn.sh \
          && sbin/stop-dfs.sh'"
        args:
          executable: /bin/bash
    tags: never, stop_hadoop_cluster

  # TODO: do not rely on shared folders (NFS and such) to access hadoop_logs
  ## Set hadoop logs timestamp
  - name: Set hadoop logs timestamp
    delegate_to: localhost
    run_once: yes
    when: "app_jar != ''"
    shell: "mv hadoop_logs hadoop_logs_`date +%d-%m-%y--%H-%M-%S`"
    args:
      chdir: "{{ default_bind_path }}/{{ bind_dir_name }}/{{ rm_container }}"
      executable: /bin/bash
    tags: never, set_hadoop_logs_timestamp
